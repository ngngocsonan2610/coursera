{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Exercise 3.2\nWelcome to the last exercise of this course. This is also the most advanced one because it somehow glues everything together you've learned. \n\nThese are the steps you will do:\n- load a data frame from cloudant/ApacheCouchDB\n- perform feature transformation by calculating minimal and maximal values of different properties on time windows (we'll explain what a time windows is later in here)\n- reduce these now twelve dimensions to three using the PCA (Principal Component Analysis) algorithm of SparkML (Spark Machine Learning) => We'll actually make use of SparkML a lot more in the next course\n- plot the dimensionality reduced data set"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import ibmos2spark\n\n# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'api_key': 'PUJMZf9PLqN4y-6NUtVlEuq6zFoWhfuecFVMYLBrkxrT',\n    'service_id': 'iam-ServiceId-9cd8e66e-3bb4-495a-807a-588692cca4d0',\n    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token'}\n\nconfiguration_name = 'os_b0f1407510994fd1b793b85137baafb8_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n\ndf = spark.read.parquet(cos.url('washing.parquet', 'courseradsnew-donotdelete-pr-1hffrnl2pprwut'))\ndf.createOrReplaceTempView('washing')\ndf.show()",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "Py4JJavaError",
                    "evalue": "An error occurred while calling o174.parquet.\n: java.io.IOException: com.ibm.stocator.thirdparty.cos.services.s3.model.AmazonS3Exception: Token retrieval from IAM service failed (Service: IAM; Status Code: 400; Error Code: null; Request ID: null), S3 Extended Request ID: null\n\tat com.ibm.stocator.fs.cos.COSAPIClient.getFileStatus(COSAPIClient.java:596)\n\tat com.ibm.stocator.fs.ObjectStoreFileSystem.getFileStatus(ObjectStoreFileSystem.java:515)\n\tat com.ibm.stocator.fs.ObjectStoreFileSystem.exists(ObjectStoreFileSystem.java:569)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:718)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:389)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:819)\nCaused by: com.ibm.stocator.thirdparty.cos.services.s3.model.AmazonS3Exception: Token retrieval from IAM service failed (Service: IAM; Status Code: 400; Error Code: null; Request ID: null), S3 Extended Request ID: null\n\tat com.ibm.stocator.thirdparty.cos.oauth.DefaultTokenProvider.retrieveToken(DefaultTokenProvider.java:160)\n\tat com.ibm.stocator.thirdparty.cos.oauth.DefaultTokenManager.retrieveToken(DefaultTokenManager.java:381)\n\tat com.ibm.stocator.thirdparty.cos.oauth.DefaultTokenManager.getToken(DefaultTokenManager.java:200)\n\tat com.ibm.stocator.thirdparty.cos.oauth.IBMOAuthSigner.sign(IBMOAuthSigner.java:64)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1164)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1030)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:742)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:716)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n\tat com.ibm.stocator.thirdparty.cos.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3921)\n\tat com.ibm.stocator.thirdparty.cos.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3868)\n\tat com.ibm.stocator.thirdparty.cos.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1227)\n\tat com.ibm.stocator.thirdparty.cos.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1202)\n\tat com.ibm.stocator.fs.cos.COSAPIClient.getFileStatusKeyBased(COSAPIClient.java:669)\n\tat com.ibm.stocator.fs.cos.COSAPIClient.getFileStatus(COSAPIClient.java:590)\n\t... 25 more\n",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-3-ced531ccd1ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'washing.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'courseradsnew-donotdelete-pr-1hffrnl2pprwut'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'washing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o174.parquet.\n: java.io.IOException: com.ibm.stocator.thirdparty.cos.services.s3.model.AmazonS3Exception: Token retrieval from IAM service failed (Service: IAM; Status Code: 400; Error Code: null; Request ID: null), S3 Extended Request ID: null\n\tat com.ibm.stocator.fs.cos.COSAPIClient.getFileStatus(COSAPIClient.java:596)\n\tat com.ibm.stocator.fs.ObjectStoreFileSystem.getFileStatus(ObjectStoreFileSystem.java:515)\n\tat com.ibm.stocator.fs.ObjectStoreFileSystem.exists(ObjectStoreFileSystem.java:569)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:718)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:389)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:819)\nCaused by: com.ibm.stocator.thirdparty.cos.services.s3.model.AmazonS3Exception: Token retrieval from IAM service failed (Service: IAM; Status Code: 400; Error Code: null; Request ID: null), S3 Extended Request ID: null\n\tat com.ibm.stocator.thirdparty.cos.oauth.DefaultTokenProvider.retrieveToken(DefaultTokenProvider.java:160)\n\tat com.ibm.stocator.thirdparty.cos.oauth.DefaultTokenManager.retrieveToken(DefaultTokenManager.java:381)\n\tat com.ibm.stocator.thirdparty.cos.oauth.DefaultTokenManager.getToken(DefaultTokenManager.java:200)\n\tat com.ibm.stocator.thirdparty.cos.oauth.IBMOAuthSigner.sign(IBMOAuthSigner.java:64)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1164)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1030)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:742)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:716)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n\tat com.ibm.stocator.thirdparty.cos.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n\tat com.ibm.stocator.thirdparty.cos.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3921)\n\tat com.ibm.stocator.thirdparty.cos.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3868)\n\tat com.ibm.stocator.thirdparty.cos.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1227)\n\tat com.ibm.stocator.thirdparty.cos.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1202)\n\tat com.ibm.stocator.fs.cos.COSAPIClient.getFileStatusKeyBased(COSAPIClient.java:669)\n\tat com.ibm.stocator.fs.cos.COSAPIClient.getFileStatus(COSAPIClient.java:590)\n\t... 25 more\n"
                    ]
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "This is the feature transformation part of this exercise. Since our table is mixing schemas from different sensor data sources we are creating new features. In other word we use existing columns to calculate new ones. We only use min and max for now, but using more advanced aggregations as we've learned in week three may improve the results. We are calculating those aggregations over a sliding window \"w\". This window is defined in the SQL statement and basically reads the table by a one by one stride in direction of increasing timestamp. Whenever a row leaves the window a new one is included. Therefore this window is called sliding window (in contrast to tubling, time or count windows). More on this can be found here: https://flink.apache.org/news/2015/12/04/Introducing-windows.html\n\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "result = spark.sql(\"\"\"\nSELECT * from (\n    SELECT\n    min(temperature) over w as min_temperature,\n    max(temperature) over w as max_temperature, \n    min(voltage) over w as min_voltage,\n    max(voltage) over w as max_voltage,\n    min(flowrate) over w as min_flowrate,\n    max(flowrate) over w as max_flowrate,\n    min(frequency) over w as min_frequency,\n    max(frequency) over w as max_frequency,\n    min(hardness) over w as min_hardness,\n    max(hardness) over w as max_hardness,\n    min(speed) over w as min_speed,\n    max(speed) over w as max_speed\n    FROM washing \n    WINDOW w AS (ORDER BY ts ROWS BETWEEN CURRENT ROW AND 10 FOLLOWING) \n)\nWHERE min_temperature is not null \nAND max_temperature is not null\nAND min_voltage is not null\nAND max_voltage is not null\nAND min_flowrate is not null\nAND max_flowrate is not null\nAND min_frequency is not null\nAND max_frequency is not null\nAND min_hardness is not null\nAND min_speed is not null\nAND max_speed is not null   \n\"\"\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Since this table contains null values also our window might contain them. In case for a certain feature all values in that window are null we obtain also null. As we can see here (in my dataset) this is the case for 9 rows."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.count()-result.count()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now we import some classes from SparkML. PCA for the actual algorithm. Vectors for the data structure expected by PCA and VectorAssembler to transform data into these vector structures."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.feature import PCA\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let's define a vector transformation helper class which takes all our input features (result.columns) and created one additional column called \"features\" which contains all our input features as one single column wrapped in \"DenseVector\" objects"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "assembler = VectorAssembler(inputCols=result.columns, outputCol=\"features\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now we actually transform the data, note that this is highly optimized code and runs really fast in contrast if we had implemented it."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "features = assembler.transform(result)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Let's have a look at how this new additional column \"features\" looks like:"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "features.rdd.map(lambda r : r.features).take(10)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Since the source data set has been prepared as a list of DenseVectors we can now apply PCA. Note that the first line again only prepares the algorithm by finding the transformation matrices (fit method)"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\nmodel = pca.fit(features)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now we can actually transform the data. Let's have a look at the first 20 rows"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "result_pca = model.transform(features).select(\"pcaFeatures\")\nresult_pca.show(truncate=False)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "So we obtained three completely new columns which we can plot now. Let run a final check if the number of rows is the same."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "result_pca.count()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Cool, this works as expected. Now we obtain a sample and read each of the three columns into a python list"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd = result_pca.rdd.sample(False,0.8)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "x = rdd.map(lambda a : a.pcaFeatures).map(lambda a : a[0]).collect()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "y = rdd.map(lambda a : a.pcaFeatures).map(lambda a : a[1]).collect()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "z = rdd.map(lambda a : a.pcaFeatures).map(lambda a : a[2]).collect()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Finally we plot the three lists and name each of them as dimension 1-3 in the plot"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n\n\n\nax.scatter(x,y,z, c='r', marker='o')\n\nax.set_xlabel('dimension1')\nax.set_ylabel('dimension2')\nax.set_zlabel('dimension3')\n\nplt.show()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "Congratulations, we are done! We can see two clusters in the data set. We can also see a third cluster which either can be outliers or a real cluster. In the next course we will actually learn how to compute clusters automatically. For now we know that the data indicates that there are two semi-stable states of the machine and sometime we see some anomalies since those data points don't fit into one of the two clusters."
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python36",
            "display_name": "Python 3.6 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}